# ROS 2 Multi-Sensor Perception System

This project implements a **real-time perception system** using **multiple USB cameras** and a **real lidar sensor**, built with the ROS 2 framework.

It supports:
- Multiple real camera feeds
- Manual camera selection via mouse click
- Real-time human detection (HOG-based)
- Real sensor data from lidar (planned or ongoing)
- Modular node-based architecture

---

## ðŸš€ Features

- ðŸ§  **Human Detection:** Detects and highlights people using OpenCV's HOG detector
- ðŸŽ¥ **Multi-Camera Support:** 3 simultaneous USB cameras
- ðŸ–±ï¸ **Interactive Selection:** Select active camera via OpenCV window click
- ðŸ“¡ **ROS 2 Integration:** Uses `rclpy`, `sensor_msgs`, `cv_bridge`, and image topics
- ðŸ” **Real Sensor Input:** Works with real hardware, not simulation

---

## ðŸ§© System Requirements

- Ubuntu 20.04 or later
- ROS 2 Foxy, Humble or Iron
- Python 3.8+
- USB cameras (at least 2 recommended)
- Optional: Real lidar sensor (Velodyne, RPLidar etc.)

---

## ðŸ“¦ Dependencies

Install required packages:

```bash
sudo apt install ros-${ROS_DISTRO}-cv-bridge ros-${ROS_DISTRO}-image-transport
pip install opencv-python
```

---

## ðŸ› ï¸ How to Run

### 1. Clone the repo and build:

```bash
cd ~/ros2_ws/src
git clone https://github.com/fatihboken/ros2-multisensor-perception.git
cd ~/ros2_ws
colcon build
source install/setup.bash
```

### 2. Run the camera node:

```bash
ros2 run camera_node camera_node
```

### 3. Click on a camera window to activate human detection for that feed.

> Detected people will be shown with green bounding boxes.

---

## ðŸ§  Future Plans

- Add real lidar point cloud processing
- Sensor fusion (camera + lidar)
- Multi-object tracking
- Map integration with SLAM

---

## ðŸ‘¤ Author

Fatih BÃ¶ken  
[GitHub](https://github.com/fatihboken)

---

## ðŸ“œ License

MIT License
